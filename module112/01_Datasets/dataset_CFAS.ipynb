{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Dataset?\n",
    "\n",
    "A dataset is a collection of data points with a common schema. The Cortex Python SDK provides transformations and visualizations to facilitate data cleaning, feature identification and feature construction. In this notebook we demonstrate how to build a dataset and how to view the contents of datasets.\n",
    "\n",
    "## How is a Dataset Built? \n",
    "First, import the Cortex library and instantiate a builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell if your token/session has expired.\n",
    "\n",
    "from cortex import Cortex\n",
    "Cortex.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cortex import Cortex\n",
    "\n",
    "builder = Cortex.client().builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Builder is the top level factory object in the Cortext Python SDK. The builder returns a factory object that is customized to handle the context for the particular class it builds. A dataset requires a collection of data to be useful, so the factory object returns a dataset builder that can take data in a number of different forms.\n",
    "\n",
    "For example, you can associate a CSV file with a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help with making this dataset distinct in class we will use an input generator here for the dataset name.  \n",
    "# This variable will be stored throughout this example.\n",
    "dataset_name1 = input(\"namespace/dataset name\")\n",
    "    \n",
    "csv_data_set_builder = builder.dataset(dataset_name1)\n",
    "\n",
    "csv_example_data_set = csv_data_set_builder.from_csv('./data/sample_large.csv').build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a dataset with JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help with making this dataset distinct in class we will use an input generator here for the dataset name.  \n",
    "# This variable will be stored throughout this example.\n",
    "dataset_name2 = input(\"namespace/dataset name\")\n",
    "\n",
    "json_data_set_builder = builder.dataset(dataset_name2)\n",
    "\n",
    "json_example_data_set = json_data_set_builder.from_json('./data/sample.json').build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help with making this dataset distinct in class we will use an input generator here for the dataset name.  \n",
    "# This variable will be stored throughout this example.\n",
    "dataset_name3 = input(\"namespace/dataset name\")\n",
    "\n",
    "# two columns of random numbers, indexed a through e\n",
    "s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "q = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "# make a data frame by composing the columns together and labeling them\n",
    "pdf = pd.DataFrame({'c1':s,'c2':q})\n",
    "\n",
    "pd_data_set_builder = builder.dataset(dataset_name3)\n",
    "\n",
    "data_frame_data_set = pd_data_set_builder.from_df(pdf).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_data_set.as_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the title and description of your datasets \n",
    "\n",
    "(this pulls the last dataset you created above from memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_example_data_set.title = 'A Title for the example csv dataset by <your name>'\n",
    "csv_example_data_set.description = 'A somewhat longer piece of text that describes the purpose of the dataset by <your name>.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_example_data_set.title = 'A Title for the example json dataset by <your name>'\n",
    "json_example_data_set.description = 'A somewhat longer piece of text that describes the purpose of the dataset by <your name>.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once constructed, you can explicitly persist a dataset, here we will use the csv_example_data_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=csv_example_data_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x.as_pandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the json_example_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=json_example_data_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y.as_pandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the `Cortex.local()` client, the dataset is persisted to the local disk. When using the Cortex client `Cortex.client()`, the dataset is persisted in Cortex.\n",
    "\n",
    "Now in the Cortex CLI you can see the persisted datasets and their relevant information by using the \"cortex datasets list\" command.  If you want to see more information about a specifice dataset use the \"cortex datasets describe <dataset_name>\" command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Feature Construction\n",
    "\n",
    "Datasets help in feature construction through the use of pipelines. Pipelines allow functions to be chained together to modify and combine columns to create and clarify new features in the dataset. To find out how to create and persist pipelines, see [Pipeline](https://docs.cortex.insights.ai/docs/cortex-python-sdk-guide/pipeline/).\n",
    "\n",
    "## View Datasets\n",
    "\n",
    "Datasets can be viewed in tables or through visualizations. \n",
    "\n",
    "### Data Dictionary\n",
    "A Dataset can generate a data dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_example_data_set.get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas DataFrame\n",
    "\n",
    "Datasets can also generate pandas DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf = json_example_data_set.as_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = csv_example_data_set.as_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas' DataFrames include several different methods for [viewing data](https://pandas.pydata.org/pandas-docs/stable/10min.html#viewing-data) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Visualizations \n",
    "\n",
    "Here are the built-in visualizations that you get with datasets. Visualizations require a dataframe. Most commonly the dataframe is constructed by running a pipeline on the data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is okay to get an error here, if the name is not defined it just means the pipeline hasnt been defined and run yet.\n",
    "clean_csv_pl.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_csv_pl = csv_example_data_set.pipeline('clean_csv_pl')\n",
    "\n",
    "def add_new_column(pipeline, df):\n",
    "    x = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "    y = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "    z = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "    pdf = pd.DataFrame({'c1':x, 'c2':y,'c3':z})\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_csv_pl.add_step(add_new_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv_df = clean_csv_pl.run(csv_example_data_set.as_pandas())\n",
    "\n",
    "cleaned_csv_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = csv_example_data_set.visuals(cleaned_csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.show_corr_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.show_corr('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.show_corr_pairs('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.show_dist('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.show_probplot('c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
