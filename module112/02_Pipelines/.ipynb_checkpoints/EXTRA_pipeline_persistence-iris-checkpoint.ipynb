{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Persistence\n",
    "\n",
    "### Persistence\n",
    "A pipeline persists its steps, context and dependencies. This example shows how those features of a pipeline work.\n",
    "\n",
    "Here we will move this pipeline from our local environment to cortex.\n",
    "\n",
    "To set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cortex import Cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a Cortex Builder instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex = Cortex.local()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_set = builder.dataset('forest_fires').title('Forest Fire Data')\\\n",
    "    .from_csv('data/ff.sample.csv').build()\n",
    "tp = data_set.pipeline('prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline's have the method `to_camel()` that produces a textual representation of the state of the pipeline.  Lets look at that now and verify that this is the right dataset with the right pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is pipeline context?\n",
    "\n",
    "A pipeline's context is a dictionary of values or functions that could be used to operate on the dataframe when each pipeline step is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_set.as_pandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.set_context('one_median', np.asscalar(df['area'].astype(float).median()))\n",
    "tp.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a step that uses the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_score(pipeline, df):\n",
    "    mu = pipeline.get_context('one_median')\n",
    "    df['area'] = df['area'] * 100 - mu # lets ask about this\n",
    "    \n",
    "tp.add_step(standard_score)\n",
    "\n",
    "tp.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecies\n",
    "\n",
    "Pipelines can be chained together, running one after another. The mechanism to do this is the pipeline dependencies function.\n",
    "\n",
    "Create another pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to a farenheit conversion \n",
    "dtp = builder.pipeline('depends_on_prep_pipeline')\n",
    "\n",
    "def diff_temp_rain(pipeline, df):\n",
    "    df['c4'] = df['temp'] - df['rain']\n",
    "    \n",
    "\n",
    "dtp.add_step(diff_temp_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C3 must exist before this step is run, so `dtp` depends on `tp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp.add_dependency(tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a second pipeline is set up, run `dtp` to cause `tp` to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builders vs Datasets\n",
    "\n",
    "Datasets can have pipelines and datasets persist between kernel invocations, so pipelines associated with a dataset are persisted. Pipelines constructed from a builder are only persisted in memory. If the kernel restarts, the builders are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = builder.dataset('sample').title('Persistence Sample Data')\\\n",
    "    .from_df(df).build()\n",
    "\n",
    "data_frame = data_set.as_pandas()\n",
    "data_frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsp = data_set.pipeline('dsp')\n",
    "dsp.reset() # see below for an explanation of pipeline reset\n",
    "dsp.add_step(add_one)\n",
    "data_set.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Python kernel is reset, all in-memory data is cleared. However, the dataset, along with its pipeline, is persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! this cell resets the kernel, wiping out everything!\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test pipeline is gone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tp\n",
    "except NameError:\n",
    "    print('No tp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we demonstrate that the `sample` dataset is persisted as is the `dsp` pipeline. In order to show how the pipeline persists, re-establish the required libraries and the Cortex builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cortex import Cortex\n",
    "\n",
    "cortex = Cortex.local()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset, using the name we previousily specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = cortex.dataset('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pipeline, again using the name we previousily specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = data_set.pipeline('dsp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe from the dataset and then run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_set.as_pandas()\n",
    "new_pipeline.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `add_one` function is still present in the `new_pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset() affects pipeline state\n",
    "\n",
    "Without parameters, reset removes all steps in the pipline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline.reset()\n",
    "\n",
    "new_pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete all context and dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl = data_set.pipeline('dsp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl.set_context('for_example','a string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = builder.pipeline('test_pipeline')\n",
    "another_new_pl.add_dependency(tp)\n",
    "\n",
    "another_new_pl.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next reset everything, including pipeline steps, dependencies, and context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl.reset(reset_deps=True, reset_context=True)\n",
    "\n",
    "another_new_pl.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
